{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.text as text\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression\n",
    "\n",
    "to do: 1,2 i 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j, \\quad x_{i,-1}=1,\\quad b=w_{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Generate a random feature vector $\\mathbf{x}$ witch 10000 samples and three feature \n",
    "such that first feature is drawn from N(0,1), second feature from  U(0,1) and third from N(1,2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "N(mu,sigma) denotes normal distribution with mean mu and standard deviation sigma. You can use ``numpy.random.normal`` and ``numpy.random.uniform`` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "def linear(x,w):\n",
    "    return x @ w\n",
    "\n",
    "x0, x1, x2 = np.random.normal(0,1,1000), np.random.uniform(0,1,1000), np.random.normal(1,2, 1000)\n",
    "x3 = np.array([1 for _ in range(1000)])\n",
    "print(x3.shape)\n",
    "x = np.column_stack((x0,x1,x2, x3))\n",
    "w = [0.2, 0.5,-0.25,1.0] \n",
    "noise = np.random.normal(0,0.1)\n",
    "# x.shape\n",
    "\n",
    "y = linear(x, w)\n",
    "y = y + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Using $\\mathbf{x}$ and weights w = [0.2, 0.5,-0.25,1.0] generate output $\\mathbf{y}$ assuming a $N(0,0.1)$ noise $\\mathbf{\\epsilon}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j+\\epsilon_i, \\quad x_{i,-1}=1,\\quad b=w_{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{1}{2}\\frac{1}{N}\\sum_{i=0}^{N-1} (y_i -  x_{ij} w_j  )^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y,x,w):\n",
    "    prediction = linear(x,w)\n",
    "    error = y - prediction\n",
    "    return -1/2 * np.mean(error * error)\n",
    "\n",
    "# def least_squere_error(self, X, y, params):\n",
    "#         shape = X.shape[0]\n",
    "#         return (1. / (2. * shape)) * (np.sum((np.dot(X, params) - y) ** 2.) \n",
    "#                                                    + self._lambda * np.dot(params.T, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the gradient of the loss function with respect to weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00059568, -0.01353965, -0.02866291, -0.0270701 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grad(y, x, w):\n",
    "    N = len(x)\n",
    "    return 1/N * np.dot(np.dot(x.T, x), w) - (1/N * np.dot(y, x))\n",
    "    # k to element tablicy w po którym róźniczkujemy \n",
    "    # 1 / N * sum([ y[i] - sum([x[i][j]* w[j] for j in range(len(x[i]))])*(-x[i][k]) for i in range(N)])\n",
    "    #  -> 1/N * (x.T * x * w) - (1 / N * y * x)\n",
    "\n",
    "grad(y, x, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write gradient function ``grad(y,x,w)``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement gradient descent for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ inf -inf -inf -inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/studia/datascience/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in multiply\n",
      "  after removing the cwd from sys.path.\n",
      "/home/jan/studia/datascience/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in matmul\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(13)\n",
    "\n",
    "\n",
    "class GradientDescent:\n",
    "    def __init__(self, x, y, w, precision = 0.0001, max_iterations=2000):\n",
    "        self.x = np.ndarray.copy(x).astype('float64')\n",
    "        self.y = np.ndarray.copy(y).astype('float64')\n",
    "        self.w = w\n",
    "        self.precision = precision\n",
    "        self.max_iterations = max_iterations\n",
    "        self.loss = np.Infinity\n",
    "        self._perform()\n",
    "\n",
    "    def _perform(self):\n",
    "        iteration = 0\n",
    "        last = np.Infinity\n",
    "        precision_satisfied = False\n",
    "        shape = self.x.shape[0]\n",
    "        while iteration < self.max_iterations and not precision_satisfied:\n",
    "                self.loss = loss(self.y, self.x, self.w)\n",
    "                self.w = grad(self.y,self.x,self.w)\n",
    "                if abs(self.loss - last) < self.precision:\n",
    "                    precision_satisfied = True\n",
    "                iteration += 1\n",
    "                \n",
    "    def get_w(self):\n",
    "        return self.w\n",
    "    \n",
    "GD = GradientDescent(x,y,w)\n",
    "w = GD.get_w()\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement gradient descent (GD).\n",
    "tego nie trzeba zrobic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement GD using pytorch. Start by just rewritting Problem 3 to use torch Tensors instead of numpy arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert frrom numpy arrays to torch tensors you can use ``torch.from_numpy()`` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement GD using pytorch automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end the variable with respect to which the gradient will be calculated, ``t_w`` in this case, must have attribute\n",
    "``requires_grad`` set to ``True`` (``t_w.require_grad=True``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch will automatically track any expression containing ``t_w`` and store its computational graph. The method ``backward()`` can be run on the final expression to back propagate the gradient e.g. ``loss.backward()``. Then the gradient is accesible as ``t_w.grad``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
