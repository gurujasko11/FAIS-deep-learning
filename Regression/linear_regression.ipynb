{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Created by Diana Janik and Jan Markiewicz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j, \\quad x_{i,-1}=1,\\quad b=w_{-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def linear(x,w):\n",
    "    return x @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Generate a random feature vector $\\mathbf{x}$ witch 10000 samples and three feature \n",
    "such that first feature is drawn from N(0,1), second feature from  U(,1) and third from N(1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.stack((np.random.normal(0, 1, (1000)), \n",
    "              np.random.uniform(0, 1, (1000)), \n",
    "              np.random.normal(1,2, (1000))), axis=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "N(mu,sigma) denotes normal distribution with mean mu and standard deviation sigma. You can use ``numpy.random.normal`` and ``numpy.random.uniform`` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Using $\\mathbf{x}$ and weights w = [0.2, 0.5,-0.25,1.0] generate output $\\mathbf{y}$ assuming a $N(0,0.1)$ noise $\\mathbf{\\epsilon}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array((0.2, 0.5, -0.25, 1.))\n",
    "ones = np.ones((x.shape[0], 1))\n",
    "x = np.concatenate((x,ones), axis = 1)\n",
    "noise = np.random.normal(0, 0.1)\n",
    "y = linear(x,w)\n",
    "y = y + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j+\\epsilon_i, \\quad x_{i,-1}=1,\\quad b=w_{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{1}{2}\\frac{1}{N}\\sum_{i=0}^{N-1} (y_i -  x_{ij} w_j  )^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLoss(y, x, w):\n",
    "    loss = np.square(y - linear(x,w))\n",
    "    loss = np.sum(loss) / (2*y.shape[0])\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the gradient of the loss function with respect to weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write gradient function ``grad(y,x,w)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00254884 -0.07115897 -0.13088844 -0.13759184]\n"
     ]
    }
   ],
   "source": [
    "def grad(y, x, w):\n",
    "    diff = (x @ w - y)\n",
    "    return np.dot(x.T, diff) / x.shape[0]\n",
    "#     return (w - (alpha/x.shape[0]) * tmp)\n",
    "gradient = grad(y, x, w)\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement gradient descent for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished gradient descent on iteration 500\n",
      "with loss equal 1.959934560652158e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.19997747,  0.50210241, -0.24998088,  1.13640725])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "\n",
    "def gradientDescent(y, x, w, maxIterations = 500, tolerance=0.0000001):\n",
    "    for i in range(maxIterations):\n",
    "        loss = getLoss(y, x, w)\n",
    "        if loss < tolerance:\n",
    "            maxIterations = i\n",
    "            break\n",
    "        gradient = grad(y, x, w)\n",
    "        w = w - alpha*gradient\n",
    "\n",
    "    print(\"finished gradient descent on iteration \" + str(maxIterations))\n",
    "    print(\"with loss equal \" + str(loss))\n",
    "    return w\n",
    "        \n",
    "gradientDescent(y, x, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished gradient descent on iteration 500\n",
      "with loss equal 1.929119346193321e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.20002551,  0.50199272, -0.2498733 ,  1.13648186])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getBatches(x, y, batchSize):\n",
    "    randomIndices = np.random.randint(1000, size=(batchSize))\n",
    "    xResult = []\n",
    "    yResult = []\n",
    "    for i in randomIndices:\n",
    "        xResult.append(x[i])\n",
    "        yResult.append(y[i])\n",
    "    return (np.asanyarray(xResult), np.asanyarray(yResult))\n",
    "\n",
    "def sgd(y, x, w, maxIterations = 500, tolerance=0.0000001, batchSize = 10):\n",
    "    for i in range(maxIterations):\n",
    "        loss = getLoss(y, x, w)\n",
    "        if loss < tolerance:\n",
    "            maxIterations = i\n",
    "            break\n",
    "        randomIndices = np.random.randint(1000, size=(batchSize))\n",
    "        selectedX = x[randomIndices]\n",
    "        selectedY = y[randomIndices]\n",
    "#         (selectedX, selectedY) = getBatches(x, y, batchSize)\n",
    "        gradient = grad(selectedY, selectedX, w)\n",
    "        w = w - alpha*gradient\n",
    "\n",
    "    print(\"finished gradient descent on iteration \" + str(maxIterations))\n",
    "    print(\"with loss equal \" + str(loss))\n",
    "    return w\n",
    "sgd(y, x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD takes: \n",
      "finished gradient descent on iteration 500\n",
      "with loss equal 2.0815189685419418e-07\n",
      "CPU times: user 32.1 ms, sys: 160 Âµs, total: 32.3 ms\n",
      "Wall time: 31.5 ms\n",
      "gradient descent takes: \n",
      "finished gradient descent on iteration 500\n",
      "with loss equal 1.959934560652158e-07\n",
      "CPU times: user 15.4 ms, sys: 0 ns, total: 15.4 ms\n",
      "Wall time: 15.3 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"SGD takes: \")\n",
    "%time tSGD = sgd(y, x, w)\n",
    "print(\"gradient descent takes: \")\n",
    "%time tGD = gradientDescent(y, x, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement SGD using pytorch. Start by just rewritting Problem 3 to use torch Tensors instead of numpy arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert frrom numpy arrays to torch tensors you can use ``torch.from_numpy()`` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def modelTorch(x):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.3724,  0.6897,  1.8082],\n",
       "         [-0.9042,  0.5787, -1.0603],\n",
       "         [ 0.8851,  0.4437,  1.1528],\n",
       "         [ 0.0732,  0.6072,  1.4597],\n",
       "         [-0.5207,  0.4177,  4.3378]]), tensor([[-0.4173],\n",
       "         [ 1.1055],\n",
       "         [-0.7998],\n",
       "         [-0.4994],\n",
       "         [-1.5507]])]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.stack((np.random.normal(0, 1, (1000)), \n",
    "              np.random.uniform(0, 1, (1000)), \n",
    "              np.random.normal(1,2, (1000))), axis=1)\n",
    "inputs = torch.from_numpy(inputs).float().to(device)\n",
    "\n",
    "targets = model(inputs)\n",
    "targets = np.asanyarray([[float(x + noise)] for x in targets])\n",
    "targets = torch.from_numpy(targets).float()\n",
    "\n",
    "w = torch.randn(1, 3, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]\n",
    "\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = nn.Linear(3, 1)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6237, grad_fn=<DivBackward0>)\n",
      "tensor(4.4343, grad_fn=<DivBackward0>)\n",
      "tensor(4.2526, grad_fn=<DivBackward0>)\n",
      "tensor(4.0785, grad_fn=<DivBackward0>)\n",
      "tensor(3.9115, grad_fn=<DivBackward0>)\n",
      "tensor(3.7515, grad_fn=<DivBackward0>)\n",
      "tensor(3.5980, grad_fn=<DivBackward0>)\n",
      "tensor(3.4508, grad_fn=<DivBackward0>)\n",
      "tensor(3.3097, grad_fn=<DivBackward0>)\n",
      "tensor(3.1745, grad_fn=<DivBackward0>)\n",
      "tensor(3.0448, grad_fn=<DivBackward0>)\n",
      "tensor(2.9204, grad_fn=<DivBackward0>)\n",
      "tensor(2.8012, grad_fn=<DivBackward0>)\n",
      "tensor(2.6869, grad_fn=<DivBackward0>)\n",
      "tensor(2.5773, grad_fn=<DivBackward0>)\n",
      "tensor(2.4722, grad_fn=<DivBackward0>)\n",
      "tensor(2.3714, grad_fn=<DivBackward0>)\n",
      "tensor(2.2748, grad_fn=<DivBackward0>)\n",
      "tensor(2.1822, grad_fn=<DivBackward0>)\n",
      "tensor(2.0934, grad_fn=<DivBackward0>)\n",
      "tensor(2.0082, grad_fn=<DivBackward0>)\n",
      "tensor(1.9266, grad_fn=<DivBackward0>)\n",
      "tensor(1.8483, grad_fn=<DivBackward0>)\n",
      "tensor(1.7733, grad_fn=<DivBackward0>)\n",
      "tensor(1.7013, grad_fn=<DivBackward0>)\n",
      "tensor(1.6323, grad_fn=<DivBackward0>)\n",
      "tensor(1.5661, grad_fn=<DivBackward0>)\n",
      "tensor(1.5027, grad_fn=<DivBackward0>)\n",
      "tensor(1.4419, grad_fn=<DivBackward0>)\n",
      "tensor(1.3835, grad_fn=<DivBackward0>)\n",
      "tensor(1.3276, grad_fn=<DivBackward0>)\n",
      "tensor(1.2740, grad_fn=<DivBackward0>)\n",
      "tensor(1.2226, grad_fn=<DivBackward0>)\n",
      "tensor(1.1733, grad_fn=<DivBackward0>)\n",
      "tensor(1.1260, grad_fn=<DivBackward0>)\n",
      "tensor(1.0807, grad_fn=<DivBackward0>)\n",
      "tensor(1.0372, grad_fn=<DivBackward0>)\n",
      "tensor(0.9955, grad_fn=<DivBackward0>)\n",
      "tensor(0.9556, grad_fn=<DivBackward0>)\n",
      "tensor(0.9173, grad_fn=<DivBackward0>)\n",
      "tensor(0.8805, grad_fn=<DivBackward0>)\n",
      "tensor(0.8453, grad_fn=<DivBackward0>)\n",
      "tensor(0.8115, grad_fn=<DivBackward0>)\n",
      "tensor(0.7791, grad_fn=<DivBackward0>)\n",
      "tensor(0.7481, grad_fn=<DivBackward0>)\n",
      "tensor(0.7183, grad_fn=<DivBackward0>)\n",
      "tensor(0.6897, grad_fn=<DivBackward0>)\n",
      "tensor(0.6623, grad_fn=<DivBackward0>)\n",
      "tensor(0.6360, grad_fn=<DivBackward0>)\n",
      "tensor(0.6109, grad_fn=<DivBackward0>)\n",
      "tensor(0.5867, grad_fn=<DivBackward0>)\n",
      "tensor(0.5636, grad_fn=<DivBackward0>)\n",
      "tensor(0.5413, grad_fn=<DivBackward0>)\n",
      "tensor(0.5200, grad_fn=<DivBackward0>)\n",
      "tensor(0.4996, grad_fn=<DivBackward0>)\n",
      "tensor(0.4800, grad_fn=<DivBackward0>)\n",
      "tensor(0.4612, grad_fn=<DivBackward0>)\n",
      "tensor(0.4432, grad_fn=<DivBackward0>)\n",
      "tensor(0.4259, grad_fn=<DivBackward0>)\n",
      "tensor(0.4094, grad_fn=<DivBackward0>)\n",
      "tensor(0.3935, grad_fn=<DivBackward0>)\n",
      "tensor(0.3783, grad_fn=<DivBackward0>)\n",
      "tensor(0.3636, grad_fn=<DivBackward0>)\n",
      "tensor(0.3496, grad_fn=<DivBackward0>)\n",
      "tensor(0.3362, grad_fn=<DivBackward0>)\n",
      "tensor(0.3233, grad_fn=<DivBackward0>)\n",
      "tensor(0.3109, grad_fn=<DivBackward0>)\n",
      "tensor(0.2991, grad_fn=<DivBackward0>)\n",
      "tensor(0.2877, grad_fn=<DivBackward0>)\n",
      "tensor(0.2768, grad_fn=<DivBackward0>)\n",
      "tensor(0.2663, grad_fn=<DivBackward0>)\n",
      "tensor(0.2563, grad_fn=<DivBackward0>)\n",
      "tensor(0.2466, grad_fn=<DivBackward0>)\n",
      "tensor(0.2374, grad_fn=<DivBackward0>)\n",
      "tensor(0.2285, grad_fn=<DivBackward0>)\n",
      "tensor(0.2200, grad_fn=<DivBackward0>)\n",
      "tensor(0.2119, grad_fn=<DivBackward0>)\n",
      "tensor(0.2041, grad_fn=<DivBackward0>)\n",
      "tensor(0.1966, grad_fn=<DivBackward0>)\n",
      "tensor(0.1894, grad_fn=<DivBackward0>)\n",
      "tensor(0.1825, grad_fn=<DivBackward0>)\n",
      "tensor(0.1758, grad_fn=<DivBackward0>)\n",
      "tensor(0.1695, grad_fn=<DivBackward0>)\n",
      "tensor(0.1634, grad_fn=<DivBackward0>)\n",
      "tensor(0.1575, grad_fn=<DivBackward0>)\n",
      "tensor(0.1519, grad_fn=<DivBackward0>)\n",
      "tensor(0.1465, grad_fn=<DivBackward0>)\n",
      "tensor(0.1414, grad_fn=<DivBackward0>)\n",
      "tensor(0.1364, grad_fn=<DivBackward0>)\n",
      "tensor(0.1317, grad_fn=<DivBackward0>)\n",
      "tensor(0.1271, grad_fn=<DivBackward0>)\n",
      "tensor(0.1227, grad_fn=<DivBackward0>)\n",
      "tensor(0.1185, grad_fn=<DivBackward0>)\n",
      "tensor(0.1145, grad_fn=<DivBackward0>)\n",
      "tensor(0.1106, grad_fn=<DivBackward0>)\n",
      "tensor(0.1069, grad_fn=<DivBackward0>)\n",
      "tensor(0.1033, grad_fn=<DivBackward0>)\n",
      "tensor(0.0999, grad_fn=<DivBackward0>)\n",
      "tensor(0.0966, grad_fn=<DivBackward0>)\n",
      "tensor(0.0935, grad_fn=<DivBackward0>)\n",
      "Training loss:  tensor(0.0919, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 0.01\n",
    "for epoch in range(num_epochs):\n",
    "        loss_total = 0\n",
    "        for xb,yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss_total += loss\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        loss_avg = loss_total/(len(train_dl))\n",
    "        print(loss_avg)\n",
    "        if loss_avg < tolerance:\n",
    "            break\n",
    "print('Training loss: ', loss_fn(model(inputs), targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement GD using pytorch automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end the variable with respect to which the gradient will be calculated, ``t_w`` in this case, must have attribute\n",
    "``requires_grad`` set to ``True`` (``t_w.require_grad=True``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch will automatically track any expression containing ``t_w`` and store its computational graph. The method ``backward()`` can be run on the final expression to back propagate the gradient e.g. ``loss.backward()``. Then the gradient is accesible as ``t_w.grad``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
