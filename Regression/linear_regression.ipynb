{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j, \\quad x_{i,-1}=1,\\quad b=w_{-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def linear(x,w):\n",
    "    return x @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Generate a random feature vector $\\mathbf{x}$ witch 10000 samples and three feature \n",
    "such that first feature is drawn from N(0,1), second feature from  U(,1) and third from N(1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.stack((np.random.normal(0, 1, (1000)), \n",
    "              np.random.uniform(0, 1, (1000)), \n",
    "              np.random.normal(1,2, (1000))), axis=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "N(mu,sigma) denotes normal distribution with mean mu and standard deviation sigma. You can use ``numpy.random.normal`` and ``numpy.random.uniform`` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Using $\\mathbf{x}$ and weights w = [0.2, 0.5,-0.25,1.0] generate output $\\mathbf{y}$ assuming a $N(0,0.1)$ noise $\\mathbf{\\epsilon}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  122.13588729 -2051.10212788  -272.19292822  -101.13199091]\n"
     ]
    }
   ],
   "source": [
    "w = np.array((0.2, 0.5, -0.25, 1.))\n",
    "ones = np.ones((x.shape[0], 1))\n",
    "x = np.concatenate((x,ones), axis = 1)\n",
    "noise = np.random.normal(0, 0.1, x.shape[0])\n",
    "y = linear(x,w)\n",
    "y = y + noise\n",
    "#set random weights\n",
    "w = np.random.normal(0,1000,(4))\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j+\\epsilon_i, \\quad x_{i,-1}=1,\\quad b=w_{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{1}{2}\\frac{1}{N}\\sum_{i=0}^{N-1} (y_i -  x_{ij} w_j  )^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLoss(y, x, w):\n",
    "    loss = np.square(y - linear(x,w))\n",
    "    loss = np.sum(loss) / (2*y.shape[0])\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the gradient of the loss function with respect to weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write gradient function ``grad(y,x,w)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  143.03142289  -872.68466279 -2482.11051852 -1387.40437993]\n"
     ]
    }
   ],
   "source": [
    "def grad(y, x, w):\n",
    "    diff = (x @ w - y)\n",
    "    return np.dot(x.T, diff) / x.shape[0]\n",
    "#     return (w - (alpha/x.shape[0]) * tmp)\n",
    "gradient = grad(y, x, w)\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement gradient descent for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0, loss=1312266.4281548972\n",
      "i=10, loss=67835.92447028619\n",
      "i=20, loss=45603.79038656082\n",
      "i=30, loss=30846.49466929855\n",
      "i=40, loss=20864.784866291964\n",
      "i=50, loss=14113.087192412471\n",
      "i=60, loss=9546.1919720036\n",
      "i=70, loss=6457.112311988214\n",
      "i=80, loss=4367.637365578803\n",
      "i=90, loss=2954.3019931750837\n",
      "i=100, loss=1998.3121267638426\n",
      "i=110, loss=1351.6739384570963\n",
      "i=120, loss=914.2833747408512\n",
      "i=130, loss=618.4293582483045\n",
      "i=140, loss=418.3116457602121\n",
      "i=150, loss=282.9506345214962\n",
      "i=160, loss=191.39150596780593\n",
      "i=170, loss=129.46027521830115\n",
      "i=180, loss=87.56956041208414\n",
      "i=190, loss=59.23438984081561\n",
      "i=200, loss=40.068283580348364\n",
      "i=210, loss=27.10419613066849\n",
      "i=220, loss=18.335197299142145\n",
      "i=230, loss=12.403785558379052\n",
      "i=240, loss=8.391737442694852\n",
      "i=250, loss=5.6779602196531345\n",
      "i=260, loss=3.842342449390154\n",
      "i=270, loss=2.6007178348506588\n",
      "i=280, loss=1.7608743181856992\n",
      "i=290, loss=1.1927983278866772\n",
      "i=300, loss=0.8085477807898732\n",
      "i=310, loss=0.5486380407804875\n",
      "i=320, loss=0.3728332866633048\n",
      "i=330, loss=0.25391772940451585\n",
      "i=340, loss=0.17348242019357907\n",
      "i=350, loss=0.11907541814102936\n",
      "i=360, loss=0.0822741439073045\n",
      "i=370, loss=0.0573815063030909\n",
      "i=380, loss=0.040543953261874106\n",
      "i=390, loss=0.02915491538957563\n",
      "i=400, loss=0.021451291036373596\n",
      "i=410, loss=0.01624050599547087\n",
      "i=420, loss=0.012715894786824687\n",
      "i=430, loss=0.010331823274643841\n",
      "finished gradient descent on iteration 432\n",
      "with loss equal 0.009957003632120952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.19867092,  0.17312994, -0.24973335,  1.17387445])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.3\n",
    "\n",
    "def gradientDescent(y, x, w, maxIterations = 500, tolerance=0.01):\n",
    "    for i in range(maxIterations):\n",
    "        loss = getLoss(y, x, w)\n",
    "        if loss < tolerance:\n",
    "            maxIterations = i\n",
    "            break\n",
    "        gradient = grad(y, x, w)\n",
    "        w = w - alpha*gradient\n",
    "        if i % 10 == 0:\n",
    "            print(\"i=\"+str(i)+\", loss=\"+str(loss))\n",
    "\n",
    "    print(\"finished gradient descent on iteration \" + str(maxIterations))\n",
    "    print(\"with loss equal \" + str(loss))\n",
    "    return w\n",
    "        \n",
    "gradientDescent(y, x, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0, loss=1312266.4281548972\n",
      "i=10, loss=168307.3226750543\n",
      "i=20, loss=51208.63851445202\n",
      "i=30, loss=34872.303851870965\n",
      "i=40, loss=25321.61575590273\n",
      "i=50, loss=15834.375795431066\n",
      "i=60, loss=12432.856309043458\n",
      "i=70, loss=17468.001216947534\n",
      "i=80, loss=15963.719787123322\n",
      "i=90, loss=7467.246713554412\n",
      "i=100, loss=2506.5141574037525\n",
      "i=110, loss=1550.5866160054206\n",
      "i=120, loss=1042.3605519160853\n",
      "i=130, loss=689.1586547404727\n",
      "i=140, loss=1035.4476679108295\n",
      "i=150, loss=679.8665317435531\n",
      "i=160, loss=226.90333055716368\n",
      "i=170, loss=283.401979829168\n",
      "i=180, loss=99.94022618576761\n",
      "i=190, loss=66.05646168383538\n",
      "i=200, loss=47.92871342633085\n",
      "i=210, loss=34.432633434397765\n",
      "i=220, loss=26.101764330770028\n",
      "i=230, loss=136.93389363718285\n",
      "i=240, loss=5.654770843269113\n",
      "i=250, loss=8.812139400334784\n",
      "i=260, loss=2.5449690810958416\n",
      "i=270, loss=1.9418411610512245\n",
      "i=280, loss=1.1694025106957628\n",
      "i=290, loss=0.9708191573837514\n",
      "i=300, loss=0.5893909776070682\n",
      "i=310, loss=0.4009049271279307\n",
      "i=320, loss=0.26823729954153863\n",
      "i=330, loss=0.20063061770946314\n",
      "i=340, loss=0.12307924443326441\n",
      "i=350, loss=0.0778092312697949\n",
      "i=360, loss=0.06373088067914358\n",
      "i=370, loss=0.07490359069545292\n",
      "i=380, loss=0.03101300570668425\n",
      "i=390, loss=0.08863231258423158\n",
      "i=400, loss=0.016888089317095716\n",
      "i=410, loss=0.012346390178911641\n",
      "i=420, loss=0.03170015259350569\n",
      "finished stochastic gradient descent on iteration 430\n",
      "with loss equal 0.009251389311611373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.21622965,  0.23483976, -0.24903316,  1.0922328 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sgd(y, x, w, maxIterations = 500, tolerance=0.01, batchSize = 10):\n",
    "    for i in range(maxIterations):\n",
    "        loss = getLoss(y, x, w)\n",
    "        if loss < tolerance:\n",
    "            maxIterations = i\n",
    "            break\n",
    "        randomIndices = np.random.randint(1000, size=(batchSize))\n",
    "        selectedX = x[randomIndices]\n",
    "        selectedY = y[randomIndices]\n",
    "        gradient = grad(selectedY, selectedX, w)\n",
    "        w = w - alpha*gradient\n",
    "        if i % 10 == 0:\n",
    "            print(\"i=\"+str(i)+\", loss=\"+str(loss))\n",
    "\n",
    "    print(\"finished stochastic gradient descent on iteration \" + str(maxIterations))\n",
    "    print(\"with loss equal \" + str(loss))\n",
    "    return w\n",
    "sgd(y, x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD takes: \n",
      "i=0, loss=1774508.9541529212\n",
      "i=10, loss=11323.975449907388\n",
      "i=20, loss=4824.496973278289\n",
      "i=30, loss=2987.113102007137\n",
      "i=40, loss=3370.72455418032\n",
      "i=50, loss=1346.2393655775008\n",
      "i=60, loss=1071.9215565907543\n",
      "i=70, loss=1217.1014062484294\n",
      "i=80, loss=868.5892651844157\n",
      "i=90, loss=282.92305406897077\n",
      "i=100, loss=196.5308352616469\n",
      "i=110, loss=175.0897587423172\n",
      "i=120, loss=108.82145176174073\n",
      "i=130, loss=66.73697168440447\n",
      "i=140, loss=69.86330556692812\n",
      "i=150, loss=28.85455056524625\n",
      "i=160, loss=37.74475724329161\n",
      "i=170, loss=13.86319448301725\n",
      "i=180, loss=10.822622404521365\n",
      "i=190, loss=7.612568718088056\n",
      "i=200, loss=7.312812972973595\n",
      "i=210, loss=3.2251331330489164\n",
      "i=220, loss=2.530378219911061\n",
      "i=230, loss=1.720699224931459\n",
      "i=240, loss=1.065385573208324\n",
      "i=250, loss=0.9025151600485497\n",
      "i=260, loss=0.5654545535066945\n",
      "i=270, loss=1.861644765256073\n",
      "i=280, loss=0.2632660108974288\n",
      "i=290, loss=0.4608242451705873\n",
      "i=300, loss=0.1132977054130163\n",
      "i=310, loss=0.1190963440830426\n",
      "i=320, loss=0.06507818243738543\n",
      "i=330, loss=0.08996332091914805\n",
      "i=340, loss=0.028026882909833115\n",
      "i=350, loss=0.022617115058824033\n",
      "i=360, loss=0.01485132504389739\n",
      "i=370, loss=0.012761345249421203\n",
      "i=380, loss=0.011978009600237254\n",
      "finished stochastic gradient descent on iteration 385\n",
      "with loss equal 0.008567582209187222\n",
      "CPU times: user 37.5 ms, sys: 3.55 ms, total: 41 ms\n",
      "Wall time: 38.2 ms\n",
      "gradient descent takes: \n",
      "i=0, loss=5022356.187138958\n",
      "i=10, loss=13699.992379969444\n",
      "i=20, loss=9126.624762695194\n",
      "i=30, loss=6173.274647745759\n",
      "i=40, loss=4175.647566731454\n",
      "i=50, loss=2824.4387591739205\n",
      "i=60, loss=1910.4717329347461\n",
      "i=70, loss=1292.2580857148203\n",
      "i=80, loss=874.09407909675\n",
      "i=90, loss=591.2450394205496\n",
      "i=100, loss=399.9239836157154\n",
      "i=110, loss=270.5130920808539\n",
      "i=120, loss=182.97866583907427\n",
      "i=130, loss=123.76977169886612\n",
      "i=140, loss=83.72046429124958\n",
      "i=150, loss=56.6308344793571\n",
      "i=160, loss=38.307220643133505\n",
      "i=170, loss=25.913000923244713\n",
      "i=180, loss=17.529464583857425\n",
      "i=190, loss=11.85878235115148\n",
      "i=200, loss=8.023093486388106\n",
      "i=210, loss=5.428606886096061\n",
      "i=220, loss=3.673678121948179\n",
      "i=230, loss=2.486632111810014\n",
      "i=240, loss=1.6837059423998704\n",
      "i=250, loss=1.1406011023030573\n",
      "i=260, loss=0.7732412147142965\n",
      "i=270, loss=0.5247564341603791\n",
      "i=280, loss=0.35667960200485455\n",
      "i=290, loss=0.24299126485815914\n",
      "i=300, loss=0.16609168365222057\n",
      "i=310, loss=0.11407627259014894\n",
      "i=320, loss=0.07889268712027987\n",
      "i=330, loss=0.05509426543639898\n",
      "i=340, loss=0.03899684764682087\n",
      "i=350, loss=0.028108442401537835\n",
      "i=360, loss=0.020743449435879458\n",
      "i=370, loss=0.01576171701848106\n",
      "i=380, loss=0.0123920385959383\n",
      "i=390, loss=0.010112764696823505\n",
      "finished gradient descent on iteration 391\n",
      "with loss equal 0.009930090919036395\n",
      "CPU times: user 18 ms, sys: 427 µs, total: 18.4 ms\n",
      "Wall time: 16.9 ms\n"
     ]
    }
   ],
   "source": [
    "#set random weights\n",
    "w = np.random.normal(0,1000,(4))\n",
    "print(\"SGD takes: \")\n",
    "%time tSGD = sgd(y, x, w)\n",
    "#set random weights\n",
    "w = np.random.normal(0,1000,(4))\n",
    "print(\"gradient descent takes: \")\n",
    "%time tGD = gradientDescent(y, x, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement SGD using pytorch. Start by just rewritting Problem 3 to use torch Tensors instead of numpy arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert frrom numpy arrays to torch tensors you can use ``torch.from_numpy()`` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement GD using pytorch automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end the variable with respect to which the gradient will be calculated, ``t_w`` in this case, must have attribute\n",
    "``requires_grad`` set to ``True`` (``t_w.require_grad=True``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch will automatically track any expression containing ``t_w`` and store its computational graph. The method ``backward()`` can be run on the final expression to back propagate the gradient e.g. ``loss.backward()``. Then the gradient is accesible as ``t_w.grad``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
