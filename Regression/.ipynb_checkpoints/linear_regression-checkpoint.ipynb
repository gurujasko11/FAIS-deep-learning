{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j, \\quad x_{i,-1}=1,\\quad b=w_{-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def linear(x,w):\n",
    "    return x @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Generate a random feature vector $\\mathbf{x}$ witch 10000 samples and three feature \n",
    "such that first feature is drawn from N(0,1), second feature from  U(,1) and third from N(1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.stack((np.random.normal(0, 1, (1000)), \n",
    "              np.random.uniform(0, 1, (1000)), \n",
    "              np.random.normal(1,2, (1000))), axis=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "N(mu,sigma) denotes normal distribution with mean mu and standard deviation sigma. You can use ``numpy.random.normal`` and ``numpy.random.uniform`` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Using $\\mathbf{x}$ and weights w = [0.2, 0.5,-0.25,1.0] generate output $\\mathbf{y}$ assuming a $N(0,0.1)$ noise $\\mathbf{\\epsilon}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array((0.2, 0.5, -0.25, 1.))\n",
    "ones = np.ones((x.shape[0], 1))\n",
    "x = np.concatenate((x,ones), axis = 1)\n",
    "noise = np.random.normal(0, 0.1)\n",
    "y = linear(x,w)\n",
    "y = y + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j+\\epsilon_i, \\quad x_{i,-1}=1,\\quad b=w_{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{1}{2}\\frac{1}{N}\\sum_{i=0}^{N-1} (y_i -  x_{ij} w_j  )^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLoss(y, x, w):\n",
    "    loss = np.square(y - linear(x,w))\n",
    "    loss = np.sum(loss) / (2*y.shape[0])\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the gradient of the loss function with respect to weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write gradient function ``grad(y,x,w)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00111374  0.01733117  0.03336937  0.03351661]\n"
     ]
    }
   ],
   "source": [
    "def grad(y, x, w):\n",
    "    diff = (x @ w - y)\n",
    "    return np.dot(x.T, diff) / x.shape[0]\n",
    "#     return (w - (alpha/x.shape[0]) * tmp)\n",
    "gradient = grad(y, x, w)\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement gradient descent for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished gradient descent on iteration 349\n",
      "with loss equal 9.899652083778778e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.20000397,  0.49846432, -0.25001459,  0.96734723])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "\n",
    "def gradientDescent(y, x, w, maxIterations = 500, tolerance=0.0000001):\n",
    "    for i in range(maxIterations):\n",
    "        loss = getLoss(y, x, w)\n",
    "        if loss < tolerance:\n",
    "            maxIterations = i\n",
    "            break\n",
    "        gradient = grad(y, x, w)\n",
    "        w = w - alpha*gradient\n",
    "\n",
    "    print(\"finished gradient descent on iteration \" + str(maxIterations))\n",
    "    print(\"with loss equal \" + str(loss))\n",
    "    return w\n",
    "        \n",
    "gradientDescent(y, x, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished gradient descent on iteration 345\n",
      "with loss equal 9.889836325315991e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.20000107,  0.49845625, -0.24999052,  0.96730586])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getBatches(x, y, batchSize):\n",
    "    randomIndices = np.random.randint(1000, size=(batchSize))\n",
    "    xResult = []\n",
    "    yResult = []\n",
    "    for i in randomIndices:\n",
    "        xResult.append(x[i])\n",
    "        yResult.append(y[i])\n",
    "    return (np.asanyarray(xResult), np.asanyarray(yResult))\n",
    "\n",
    "def sgd(y, x, w, maxIterations = 500, tolerance=0.0000001, batchSize = 10):\n",
    "    for i in range(maxIterations):\n",
    "        loss = getLoss(y, x, w)\n",
    "        if loss < tolerance:\n",
    "            maxIterations = i\n",
    "            break\n",
    "        randomIndices = np.random.randint(1000, size=(batchSize))\n",
    "        selectedX = x[randomIndices]\n",
    "        selectedY = y[randomIndices]\n",
    "#         (selectedX, selectedY) = getBatches(x, y, batchSize)\n",
    "        gradient = grad(selectedY, selectedX, w)\n",
    "        w = w - alpha*gradient\n",
    "\n",
    "    print(\"finished gradient descent on iteration \" + str(maxIterations))\n",
    "    print(\"with loss equal \" + str(loss))\n",
    "    return w\n",
    "sgd(y, x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD takes: \n",
      "finished gradient descent on iteration 341\n",
      "with loss equal 9.818739529034289e-08\n",
      "CPU times: user 26.9 ms, sys: 250 Âµs, total: 27.2 ms\n",
      "Wall time: 26.4 ms\n",
      "gradient descent takes: \n",
      "finished gradient descent on iteration 349\n",
      "with loss equal 9.899652083778778e-08\n",
      "CPU times: user 11.4 ms, sys: 0 ns, total: 11.4 ms\n",
      "Wall time: 11.3 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"SGD takes: \")\n",
    "%time tSGD = sgd(y, x, w)\n",
    "print(\"gradient descent takes: \")\n",
    "%time tGD = gradientDescent(y, x, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement SGD using pytorch. Start by just rewritting Problem 3 to use torch Tensors instead of numpy arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert frrom numpy arrays to torch tensors you can use ``torch.from_numpy()`` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement GD using pytorch automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end the variable with respect to which the gradient will be calculated, ``t_w`` in this case, must have attribute\n",
    "``requires_grad`` set to ``True`` (``t_w.require_grad=True``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch will automatically track any expression containing ``t_w`` and store its computational graph. The method ``backward()`` can be run on the final expression to back propagate the gradient e.g. ``loss.backward()``. Then the gradient is accesible as ``t_w.grad``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
